{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "236d6de5-58bd-4408-addc-0f71f52bbc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Audio, DatasetDict, ClassLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "454c913d-2313-461e-95c2-4caa8a63679b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_labels(batch):\n",
    "    batch[\"emotion\"] = [sentiment for sentiment in batch[\"emotion\"]]\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b464c60c-32bf-4743-98d2-81318877cbb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8fdac13b19f49068cd69e2f77f8a009",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1441 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "english_dataset = load_dataset(\"./dataset/ravd\", data_dir=\"./\", split=\"train\")\n",
    "features = english_dataset.features.copy()\n",
    "features[\"emotion\"] = ClassLabel(names=['happy','neutral','angry','sad','fearful','disgust','calm','surprised'])\n",
    "english_dataset = english_dataset.map(adjust_labels, batched=True, features=features)\n",
    "english_dataset = english_dataset.train_test_split(test_size=0.2,stratify_by_column=\"emotion\")\n",
    "test_data_split = english_dataset[\"test\"].train_test_split(test_size=0.5,stratify_by_column=\"emotion\")\n",
    "english_dataset = DatasetDict({\n",
    "    \"train\": english_dataset[\"train\"],\n",
    "    \"test\": test_data_split[\"test\"],\n",
    "    \"val\": test_data_split[\"train\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7726a3e0-5a53-4ffb-ab2f-44df92d766ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoFeatureExtractor\n",
    "\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-large-xlsr-53\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bb5ff1d-95be-4ab7-824e-41650ddaff81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    audio_arrays = [x[\"array\"] for x in examples[\"audio\"]]\n",
    "    inputs = feature_extractor(\n",
    "        audio_arrays, sampling_rate=feature_extractor.sampling_rate\n",
    "    )\n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18489bce-e39a-4788-a2c7-d6eaa19fa4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_dataset = english_dataset.cast_column(\"audio\", Audio(sampling_rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7478321-c2c8-4b41-b0d9-b6f274caed9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = english_dataset[\"train\"].features[\"emotion\"].names\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = str(i)\n",
    "    id2label[str(i)] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eddf2241-9dbc-41b9-9b20-455703070e7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b0edd3fbe3841efb346debcc486e23e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1152 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68b8d518d8164cd990510f8b49dce08b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/144 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5ef44033c6743318027ba1b8c96ce16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/144 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoded_english_dataset = english_dataset.map(preprocess_function, remove_columns=\"audio\", batched=True)\n",
    "encoded_english_dataset = encoded_english_dataset.rename_column(\"emotion\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6dc3eab0-86a0-406b-91d1-ce9a66922fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ada868f8-be52-4b42-823c-f29984daeb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=eval_pred.label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0871c74a-d2f6-4470-8dfe-03cb24c3596d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-large-xlsr-53 and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForAudioClassification, TrainingArguments, Trainer\n",
    "\n",
    "num_labels = len(label2id)\n",
    "model = AutoModelForAudioClassification.from_pretrained(\n",
    "    \"facebook/wav2vec2-large-xlsr-53\", num_labels=num_labels, label2id=label2id, id2label=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "018f6fcb-0dc6-4c33-a460-2b19fd73fa4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wkwon\\anaconda3\\envs\\dlnn\\Lib\\site-packages\\accelerate\\accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3600' max='3600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3600/3600 21:01, Epoch 25/25]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.083900</td>\n",
       "      <td>2.070834</td>\n",
       "      <td>0.131944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.049400</td>\n",
       "      <td>1.924896</td>\n",
       "      <td>0.277778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.769600</td>\n",
       "      <td>1.731799</td>\n",
       "      <td>0.270833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.670500</td>\n",
       "      <td>1.446478</td>\n",
       "      <td>0.465278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.093100</td>\n",
       "      <td>0.590278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.826000</td>\n",
       "      <td>0.863294</td>\n",
       "      <td>0.680556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.560400</td>\n",
       "      <td>0.664508</td>\n",
       "      <td>0.798611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.510400</td>\n",
       "      <td>0.691342</td>\n",
       "      <td>0.805556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.405700</td>\n",
       "      <td>0.535560</td>\n",
       "      <td>0.812500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.358900</td>\n",
       "      <td>0.476575</td>\n",
       "      <td>0.861111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.285400</td>\n",
       "      <td>0.450121</td>\n",
       "      <td>0.868056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.172900</td>\n",
       "      <td>0.611092</td>\n",
       "      <td>0.881944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.057200</td>\n",
       "      <td>0.676491</td>\n",
       "      <td>0.868056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.255700</td>\n",
       "      <td>0.451505</td>\n",
       "      <td>0.909722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.144600</td>\n",
       "      <td>0.505261</td>\n",
       "      <td>0.902778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>0.561510</td>\n",
       "      <td>0.902778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.100800</td>\n",
       "      <td>0.387582</td>\n",
       "      <td>0.930556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.190000</td>\n",
       "      <td>0.788158</td>\n",
       "      <td>0.868056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.092300</td>\n",
       "      <td>0.683320</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.005700</td>\n",
       "      <td>0.641264</td>\n",
       "      <td>0.895833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.052000</td>\n",
       "      <td>0.434833</td>\n",
       "      <td>0.923611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.069200</td>\n",
       "      <td>0.701157</td>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.135800</td>\n",
       "      <td>0.657787</td>\n",
       "      <td>0.881944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.631348</td>\n",
       "      <td>0.909722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.035400</td>\n",
       "      <td>0.597159</td>\n",
       "      <td>0.909722</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3600, training_loss=0.5487772062675665, metrics={'train_runtime': 1266.9957, 'train_samples_per_second': 22.731, 'train_steps_per_second': 2.841, 'total_flos': 3.5550352964020173e+18, 'train_loss': 0.5487772062675665, 'epoch': 25.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"english_new_emotion_model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=0.0001,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=25,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    push_to_hub=False,\n",
    "    \n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_english_dataset[\"train\"].with_format(\"torch\"),\n",
    "    eval_dataset=encoded_english_dataset[\"val\"].with_format(\"torch\"),\n",
    "    tokenizer=feature_extractor,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b534730-f7ae-4c14-955c-03a5d9805e4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='150' max='36' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [36/36 00:16]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.572293221950531,\n",
       " 'eval_accuracy': 0.9097222222222222,\n",
       " 'eval_runtime': 2.4581,\n",
       " 'eval_samples_per_second': 58.582,\n",
       " 'eval_steps_per_second': 14.646,\n",
       " 'epoch': 25.0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(encoded_english_dataset[\"test\"].with_format(\"torch\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db5a6484-be31-49b4-92c4-c9513f92088a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english_new_emotion_model\\checkpoint-2448\n"
     ]
    }
   ],
   "source": [
    "best_ckpt_path = trainer.state.best_model_checkpoint\n",
    "print(best_ckpt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2cd1f1-c2a4-490e-a1b3-f893b8dd1896",
   "metadata": {},
   "source": [
    "### Test against emo dataset for data with common labels only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3fdea19c-772e-4f3d-b77e-f2ea7fb5b69e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d36886f04b6462799efbee6e033508b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/455 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26917639f02a4b56a4b5582d94ed21d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4aec7e54c1445698cb4de348534d0c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/454 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "emo_dataset = load_dataset(\"./dataset/ravd_model_test_using_emo\", data_dir=\"./\", split=\"train\")\n",
    "features = emo_dataset.features.copy()\n",
    "features[\"emotion\"] = ClassLabel(names=['happy','neutral','angry','sad','fearful','disgust','calm','surprised'])\n",
    "emo_dataset = emo_dataset.map(adjust_labels, batched=True, features=features)\n",
    "emo_dataset = DatasetDict({\n",
    "    \"test\": emo_dataset,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab967889-5b34-4ff0-a7ed-952801938f5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5c2e7ee6e3f40faaf6603fa2f1a3e20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/454 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoFeatureExtractor\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-large-xlsr-53\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    audio_arrays = [x[\"array\"] for x in examples[\"audio\"]]\n",
    "    inputs = feature_extractor(\n",
    "        audio_arrays, sampling_rate=feature_extractor.sampling_rate\n",
    "    )\n",
    "    return inputs\n",
    "    \n",
    "emo_dataset = emo_dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "encoded_emo_dataset = emo_dataset.map(preprocess_function, remove_columns=\"audio\", batched=True)\n",
    "encoded_emo_dataset = encoded_emo_dataset.rename_column(\"emotion\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1eae7abd-66d1-4cf2-a57c-758719745280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 3.400320529937744,\n",
       " 'eval_accuracy': 0.5154185022026432,\n",
       " 'eval_runtime': 9.0468,\n",
       " 'eval_samples_per_second': 50.184,\n",
       " 'eval_steps_per_second': 12.601,\n",
       " 'epoch': 25.0}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(encoded_emo_dataset[\"test\"].with_format(\"torch\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
