{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "315f2171-ee3a-4265-8d49-27ef2d691a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Audio, DatasetDict, ClassLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9a89c13-1051-4ea2-81f0-00a00fc0e6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_labels(batch):\n",
    "    batch[\"emotion\"] = [sentiment for sentiment in batch[\"emotion\"]]\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "202b6a6c-fbfe-4378-b791-21be486af5ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31ab924b23854bb38b71bab94f1a4baf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/536 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "german_dataset = load_dataset(\"./dataset/emo\", data_dir=\"./\", split=\"train\")\n",
    "features = german_dataset.features.copy()\n",
    "features[\"emotion\"] = ClassLabel(names=[ 'happy','neutral','angry','sad','fearful','boredom','disgust'])\n",
    "german_dataset = german_dataset.map(adjust_labels, batched=True, features=features)\n",
    "german_dataset = german_dataset.train_test_split(test_size=0.2,stratify_by_column=\"emotion\")\n",
    "test_data_split = german_dataset[\"test\"].train_test_split(test_size=0.5,stratify_by_column=\"emotion\")\n",
    "german_dataset = DatasetDict({\n",
    "    \"train\": german_dataset[\"train\"],\n",
    "    \"test\": test_data_split[\"test\"],\n",
    "    \"val\": test_data_split[\"train\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57ce3aae-c275-4a36-84dc-ffc8e3b67e34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['audio', 'emotion'],\n",
       "        num_rows: 428\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['audio', 'emotion'],\n",
       "        num_rows: 54\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['audio', 'emotion'],\n",
       "        num_rows: 53\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "german_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "888e1c14-f455-4425-931a-3c6620b065a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'audio': {'path': 'C:\\\\Users\\\\wkwon\\\\Documents\\\\Neural Networks\\\\WenYue\\\\deep-learning-project\\\\dataset\\\\emo\\\\data\\\\15a07Fb.wav',\n",
       "  'array': array([ 9.15527344e-05,  1.52587891e-04, -7.62939453e-04, ...,\n",
       "          4.57763672e-04, -1.46484375e-03, -5.18798828e-04]),\n",
       "  'sampling_rate': 16000},\n",
       " 'emotion': 0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "german_dataset[\"train\"][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03be7293-1341-4b03-a6cd-1209ae0ea053",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'audio': {'path': 'C:\\\\Users\\\\wkwon\\\\Documents\\\\Neural Networks\\\\WenYue\\\\deep-learning-project\\\\dataset\\\\emo\\\\data\\\\10a02Wa.wav',\n",
       "  'array': array([-0.00057983, -0.0005188 , -0.00073242, ..., -0.00131226,\n",
       "         -0.00180054, -0.00143433]),\n",
       "  'sampling_rate': 16000},\n",
       " 'emotion': 2}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "german_dataset[\"train\"][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6eb5275d-39be-4cd1-a5af-2c156590449e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'audio': {'path': 'C:\\\\Users\\\\wkwon\\\\Documents\\\\Neural Networks\\\\WenYue\\\\deep-learning-project\\\\dataset\\\\emo\\\\data\\\\16b01Wb.wav',\n",
       "  'array': array([ 2.13623047e-04,  7.32421875e-04,  4.27246094e-04, ...,\n",
       "         -1.83105469e-04, -9.15527344e-05,  0.00000000e+00]),\n",
       "  'sampling_rate': 16000},\n",
       " 'emotion': 2}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "german_dataset[\"train\"][2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9f35bff-10c9-4df7-aa59-7d58f2d80d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoFeatureExtractor\n",
    "\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-large-xlsr-53\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c531bf6d-e827-4f3b-bb43-dd080979937b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    audio_arrays = [x[\"array\"] for x in examples[\"audio\"]]\n",
    "    inputs = feature_extractor(\n",
    "        audio_arrays, sampling_rate=feature_extractor.sampling_rate\n",
    "    )\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d8612bc-6c03-4fa5-9974-38e5bd1db82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "german_dataset = german_dataset.cast_column(\"audio\", Audio(sampling_rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78b2457c-457f-4034-a591-6b05a0d6590d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e66b335517143b98bdbbae6a4ecae37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/428 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67ee95ede72247b3a7da4e91950ee0cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/54 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c9e8ee24f1b45d7a8e13cd22da36ca3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/53 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoded_german_dataset = german_dataset.map(preprocess_function, remove_columns=\"audio\", batched=True)\n",
    "encoded_german_dataset = encoded_german_dataset.rename_column(\"emotion\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6180c5f2-dbe5-48ef-81d8-bef86df07fa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'input_values', 'attention_mask'],\n",
       "        num_rows: 428\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'input_values', 'attention_mask'],\n",
       "        num_rows: 54\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['label', 'input_values', 'attention_mask'],\n",
       "        num_rows: 53\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_german_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "834cb20d-a88e-4494-b6dc-00d368e25638",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "label = {\n",
    "    'happy' : 0,\n",
    "    'neutral' : 1,\n",
    "    'angry' : 2,\n",
    "    'sad' : 3,\n",
    "    'fearful': 4,\n",
    "    'boredom' : 5,\n",
    "    'disgust' : 6,\n",
    "}\n",
    "\n",
    "label_id = {\n",
    "    0 : 'happy',\n",
    "    1 : 'neutral',\n",
    "    2 : 'angry',\n",
    "    3 : 'sad',\n",
    "    4 : 'fearful',\n",
    "    5 : 'boredom',\n",
    "    6 : 'disgust',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb44aa5d-c6a0-4277-8815-82cf9339772d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce07112c-eb94-4117-a6f1-f30bd2b00bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=eval_pred.label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8063b0f8-08c8-4ded-86ee-1d93971015c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-large-xlsr-53 and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForAudioClassification, TrainingArguments, Trainer\n",
    "\n",
    "num_labels = len(label)\n",
    "model = AutoModelForAudioClassification.from_pretrained(\n",
    "    \"facebook/wav2vec2-large-xlsr-53\", num_labels=num_labels, label2id=label, id2label=label_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "362b0196-21c6-46f5-afe5-dc18f7f43bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wkwon\\anaconda3\\envs\\dlnn\\Lib\\site-packages\\accelerate\\accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1060' max='1060' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1060/1060 07:12, Epoch 19/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.937600</td>\n",
       "      <td>1.926298</td>\n",
       "      <td>0.245283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.467500</td>\n",
       "      <td>1.290713</td>\n",
       "      <td>0.566038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.870800</td>\n",
       "      <td>0.704629</td>\n",
       "      <td>0.698113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.648200</td>\n",
       "      <td>0.424657</td>\n",
       "      <td>0.886792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.372300</td>\n",
       "      <td>0.158696</td>\n",
       "      <td>0.981132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.083700</td>\n",
       "      <td>0.187771</td>\n",
       "      <td>0.943396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.035900</td>\n",
       "      <td>0.194751</td>\n",
       "      <td>0.943396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.033300</td>\n",
       "      <td>0.006215</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.016600</td>\n",
       "      <td>0.008997</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.141700</td>\n",
       "      <td>0.003631</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.028900</td>\n",
       "      <td>0.003609</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1060, training_loss=0.5308463309803662, metrics={'train_runtime': 433.3331, 'train_samples_per_second': 19.754, 'train_steps_per_second': 2.446, 'total_flos': 9.818516611860818e+17, 'train_loss': 0.5308463309803662, 'epoch': 19.81})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"german_emotion_model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=0.0001,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=20,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    push_to_hub=False,\n",
    "    \n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_german_dataset[\"train\"].with_format(\"torch\"),\n",
    "    eval_dataset=encoded_german_dataset[\"val\"].with_format(\"torch\"),\n",
    "    tokenizer=feature_extractor,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d37af0c-ab5c-4a8d-be8c-7d00cbeeabad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='278' max='14' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14/14 00:23]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.09691314399242401,\n",
       " 'eval_accuracy': 0.9814814814814815,\n",
       " 'eval_runtime': 1.3197,\n",
       " 'eval_samples_per_second': 40.918,\n",
       " 'eval_steps_per_second': 10.608,\n",
       " 'epoch': 19.81}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(encoded_german_dataset[\"test\"].with_format(\"torch\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d0b5173-2e3a-47e6-978c-a012bbbfa105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "german_emotion_model\\checkpoint-1060\n"
     ]
    }
   ],
   "source": [
    "best_ckpt_path = trainer.state.best_model_checkpoint\n",
    "print(best_ckpt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a39bbc-9c78-4b9c-bbed-d1ef6d96d39b",
   "metadata": {},
   "source": [
    "### Testing against RAVD dataset for data with common labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15cf3457-3477-4de1-ad4a-f6c4ce3e6ed8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6d8f2b2f04b4b5b8f377d2c89263f7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1057 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ravd_dataset = load_dataset(\"./dataset/emo_model_test_using_ravd\", data_dir=\"./\", split=\"train\")\n",
    "features = ravd_dataset.features.copy()\n",
    "features[\"emotion\"] = ClassLabel(names=[ 'happy','neutral','angry','sad','fearful','boredom','disgust'])\n",
    "ravd_dataset = ravd_dataset.map(adjust_labels, batched=True, features=features)\n",
    "ravd_dataset = DatasetDict({\n",
    "    \"test\": ravd_dataset,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a961102-fe06-427e-a5c0-fc97d6aacd9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a9456c1b8f24dcdb40860970616299b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1056 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoFeatureExtractor\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-large-xlsr-53\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    audio_arrays = [x[\"array\"] for x in examples[\"audio\"]]\n",
    "    inputs = feature_extractor(\n",
    "        audio_arrays, sampling_rate=feature_extractor.sampling_rate\n",
    "    )\n",
    "    return inputs\n",
    "    \n",
    "ravd_dataset = ravd_dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "encoded_ravd_dataset = ravd_dataset.map(preprocess_function, remove_columns=\"audio\", batched=True)\n",
    "encoded_ravd_dataset = encoded_ravd_dataset.rename_column(\"emotion\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d4d91785-aaf6-4852-9bcb-fd0dc51b8459",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 2.9718003273010254,\n",
       " 'eval_accuracy': 0.5350378787878788,\n",
       " 'eval_runtime': 15.6417,\n",
       " 'eval_samples_per_second': 67.512,\n",
       " 'eval_steps_per_second': 16.878,\n",
       " 'epoch': 19.81}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(encoded_ravd_dataset[\"test\"].with_format(\"torch\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3debc731-d476-44b8-98ab-0ed19017b96a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
